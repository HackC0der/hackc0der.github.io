<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="HackC0der">
    
    <!-- Completely eliminate flash of wrong theme -->
    <script>
        (function() {
            const THEME_KEY = "REDEFINE-THEME-STATUS";
            const DARK = "dark", LIGHT = "light";
            
            // Get preferred theme
            function getTheme() {
                try {
                    const saved = localStorage.getItem(THEME_KEY);
                    if (saved) {
                        const { isDark } = JSON.parse(saved);
                        return isDark ? DARK : LIGHT;
                    }
                } catch (e) {}
                
                return matchMedia("(prefers-color-scheme: dark)").matches ? DARK : LIGHT;
            }
            
            // Apply theme to document
            function applyTheme(theme) {
                const isDark = theme === DARK;
                const root = document.documentElement;
                
                // Set data attribute for CSS variables
                root.setAttribute("data-theme", theme);
                
                // Set classes for compatibility
                root.classList.add(theme);
                root.classList.remove(isDark ? LIGHT : DARK);
                root.style.colorScheme = theme;
            }
            
            // Initial application
            const theme = getTheme();
            applyTheme(theme);
            
            // Listen for system preference changes
            matchMedia("(prefers-color-scheme: dark)").addEventListener("change", ({ matches }) => {
                // Only update if using system preference (no localStorage entry)
                if (!localStorage.getItem(THEME_KEY)) {
                    applyTheme(matches ? DARK : LIGHT);
                }
            });
            
            // Set body classes once DOM is ready
            if (document.readyState !== "loading") {
                document.body.classList.add(theme + "-mode");
            } else {
                document.addEventListener("DOMContentLoaded", () => {
                    document.body.classList.add(theme + "-mode");
                    document.body.classList.remove((theme === DARK ? LIGHT : DARK) + "-mode");
                });
            }
        })();
    </script>
    
    <!-- Critical CSS to prevent flash -->
    <style>
        :root[data-theme="dark"] {
            --background-color: #202124;
            --background-color-transparent: rgba(32, 33, 36, 0.6);
            --second-background-color: #2d2e32;
            --third-background-color: #34353a;
            --third-background-color-transparent: rgba(32, 33, 36, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #ffffff;
            --second-text-color: #eeeeee;
            --third-text-color: #bebec6;
            --fourth-text-color: #999999;
            --default-text-color: #bebec6;
            --invert-text-color: #373D3F;
            --border-color: rgba(255, 255, 255, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(255, 255, 255, 0.08);
            --shadow-color-2: rgba(255, 255, 255, 0.05);
        }
        
        :root[data-theme="light"] {
            --background-color: #fff;
            --background-color-transparent: rgba(255, 255, 255, 0.6);
            --second-background-color: #f8f8f8;
            --third-background-color: #f2f2f2;
            --third-background-color-transparent: rgba(241, 241, 241, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #16171a;
            --second-text-color: #2f3037;
            --third-text-color: #5e5e5e;
            --fourth-text-color: #eeeeee;
            --default-text-color: #373D3F;
            --invert-text-color: #bebec6;
            --border-color: rgba(0, 0, 0, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(0, 0, 0, 0.08);
            --shadow-color-2: rgba(0, 0, 0, 0.05);
        }
        
        body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
        
        /* Apply body classes as soon as DOM is ready */
        :root[data-theme="dark"] body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
    </style>
    
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://HackC0der.github.io/2026/02/13/github-repos-microgpt/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="[Github-Repos] microgpt">
<meta property="og:url" content="https://hackc0der.github.io/2026/02/13/Github-Repos-microgpt/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hackc0der.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2026-02-13T01:09:47.000Z">
<meta property="article:modified_time" content="2026-02-14T02:14:27.710Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Interesting Github Repos">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hackc0der.github.io/images/redefine-og.webp">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/%E6%A9%98%E7%8C%AB.ico" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/%E6%A9%98%E7%8C%AB.ico">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/%E6%A9%98%E7%8C%AB.ico">
    <!--- Page Info-->
    
    <title>
        
            [Github-Repos] microgpt | HackC0der&#39;s Blog
        
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/tailwind.css">

    

    
<link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/fonts/Geist/geist.css">

    <!--- Font Part-->
    
    
    
        <link href="" rel="stylesheet">
    
    
    
    
        
<script src="/js/build/libs/anime.min.js"></script>

    

    <script id="hexo-configurations">
    window.config = {"hostname":"hackc0der.github.io","root":"/","language":"zh-CN","path":"search.json"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":true,"title_alignment":"left","headings_top_spacing":{"h1":"2.6rem","h2":"2.2rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":true,"list":[]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":true,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1300px","sidebar_width":"400px","hover":{"shadow":true,"scale":true},"scroll_progress":{"bar":true,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":true,"custom_message":null},"side_tools":{"gear_rotation":true,"auto_expand":true},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"What you have done does make who you are...","subtitle":{"text":["明月几时有，把酒问青天","不知天上宫阙，今夕是何年？","我欲乘风归去，又恐琼楼玉宇，高处不胜寒。","起舞弄清影，何似在人间！","转朱阁，低绮户，照无眠。","不应有恨，何事长向别时圆？","人有悲欢离合，月有阴晴圆缺，此事古难全。","但愿人长久，千里共婵娟。","我步入丛林，因为我希望活得有意义…","我希望活得深刻，汲取生命所有的精髓！","把非生命的一切全都击溃…","以免在我生命终结时，发现自己从来没有活过…"],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/HackC0der","instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.5","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"CSDN":{"path":"https://blog.csdn.net/Mr_Fmnwon?type=blog","icon":"fa-sharp fa-solid fa-browser"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2025/11/27 23:36:22"};
    window.lang_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 7.3.0"></head>



<body>
	<div class="progress-bar-container">
	
	<span class="scroll-progress-bar"></span>
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<style>
    :root {
        --preloader-background-color: #fff;
        --preloader-text-color: #000;
    }

    @media (prefers-color-scheme: dark) {
        :root {
            --preloader-background-color: #202124;
            --preloader-text-color: #fff;
        }
    }

    @media (prefers-color-scheme: light) {
        :root {
            --preloader-background-color: #fff;
            --preloader-text-color: #000;
        }
    }

    @media (max-width: 600px) {
        .ml13 {
            font-size: 2.6rem !important; /* Adjust this value as needed */
        }
    }

    .preloader {
        display: flex;
        flex-direction: column;
        gap: 1rem; /* Tailwind 'gap-4' is 1rem */
        align-items: center;
        justify-content: center;
        position: fixed;
        padding: 12px;
        top: 0;
        right: 0;
        bottom: 0;
        left: 0;
        width: 100vw;
        height: 100vh; /* 'h-screen' is 100% of the viewport height */
        background-color: var(--preloader-background-color);
        z-index: 1100; /* 'z-[1100]' sets the z-index */
        transition: opacity 0.2s ease-in-out;
    }

    .ml13 {
        font-size: 3.2rem;
        /* text-transform: uppercase; */
        color: var(--preloader-text-color);
        letter-spacing: -1px;
        font-weight: 500;
        font-family: 'Chillax-Variable', sans-serif;
        text-align: center;
    }

    .ml13 .word {
        display: inline-flex;
        flex-wrap: wrap;
        white-space: nowrap;
    }

    .ml13 .letter {
        display: inline-block;
        line-height: 1em;
    }
</style>

<div class="preloader">
    <h2 class="ml13">
        HackC0der&#39;s Blog
    </h2>
    <script>
        var textWrapper = document.querySelector('.ml13');
        // Split text into words
        var words = textWrapper.textContent.trim().split(' ');

        // Clear the existing content
        textWrapper.innerHTML = '';

        // Wrap each word and its letters in spans
        words.forEach(function(word) {
            var wordSpan = document.createElement('span');
            wordSpan.classList.add('word');
            wordSpan.innerHTML = word.replace(/\S/g, "<span class='letter'>$&</span>");
            textWrapper.appendChild(wordSpan);
            textWrapper.appendChild(document.createTextNode(' ')); // Add space between words
        });

        var animation = anime.timeline({ loop: true })
            .add({
                targets: '.ml13 .letter',
                translateY: [20, 0],
                translateZ: 0,
                opacity: [0, 1],
                filter: ['blur(5px)', 'blur(0px)'],
                easing: "easeOutExpo",
                duration: 1200,
                delay: (el, i) => 300 + 20 * i,
            })
            .add({
                targets: '.ml13 .letter',
                translateY: [0, -20],
                opacity: [1, 0],
                filter: ['blur(0px)', 'blur(5px)'],
                easing: "easeInExpo",
                duration: 1000,
                delay: (el, i) => 15 * i,
                complete: function() {
                    hidePreloader();
                }
            }, '-=700');


        let themeStatus = JSON.parse(localStorage.getItem('REDEFINE-THEME-STATUS'))?.isDark;

        // If the theme status is not found in local storage, check the preferred color scheme
        if (themeStatus === undefined || themeStatus === null) {
            if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                themeStatus = 'dark';
            } else {
                themeStatus = 'light';
            }
        }

        // Now you can use the themeStatus variable in your code
        if (themeStatus) {
            document.documentElement.style.setProperty('--preloader-background-color', '#202124');
            document.documentElement.style.setProperty('--preloader-text-color', '#fff');
        } else {
            document.documentElement.style.setProperty('--preloader-background-color', '#fff');
            document.documentElement.style.setProperty('--preloader-text-color', '#000');
        }

        window.addEventListener('load', function () {
            setTimeout(hidePreloader, 5000); // Call hidePreloader after 5000 milliseconds if not already called by animation
        });

        function hidePreloader() {
            var preloader = document.querySelector('.preloader');
            preloader.style.opacity = '0';
            setTimeout(function () {
                preloader.style.display = 'none';
            }, 200);
        }
    </script>
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                HackC0der&#39;s Blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    首页
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   target="_blank" rel="noopener" href="https://blog.csdn.net/Mr_Fmnwon?type=blog"
                                        >
                                    <i class="fa-sharp fa-solid fa-browser fa-fw"></i>
                                    CSDN
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                首页
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           target="_blank" rel="noopener" href="https://blog.csdn.net/Mr_Fmnwon?type=blog"
                        >
                            <span>
                                CSDN
                            </span>
                            
                                <i class="fa-sharp fa-solid fa-browser fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">4</div>
        <div class="label text-third-text-color text-sm">标签</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">0</div>
        <div class="label text-third-text-color text-sm">分类</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">6</div>
        <div class="label text-third-text-color text-sm">文章</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">[Github-Repos] microgpt</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/redefine-avatar.svg">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">HackC0der</span>
					
					<span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv1</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2026-02-13 09:09:47</span>
        <span class="mobile">2026-02-13 09:09:47</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2026-02-14 10:14:27</span>
            <span class="mobile">2026-02-14 10:14:27</span>
            <span class="hover-info">更新</span>
        </span>
    

    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Interesting-Github-Repos/">Interesting Github Repos</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>7.3k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>35 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<p>近几天刷短视频的时候看到了一个标题党：《243行代码复刻ChatGPT，AI大神卡帕西下场发布开源神级项目，零依赖+纯Python，新手也能经轻松上手复现..》</p>
<p>wtf？这激起了我的兴趣——毕竟250行的代码一点也不费时间，遂想一探究竟</p>
<p>原仓库地址：<a class="link" target="_blank" rel="noopener" href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95">https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p>由于墙的原因，这里找了一个镜像仓库：<a class="link" target="_blank" rel="noopener" href="https://github.com/kibotu/karpathy-microgpt">GitHub - kibotu&#x2F;karpathy-microgpt<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<img lazyload src="/images/loading.svg" data-src="/2026/02/13/Github-Repos-microgpt/image-20260213092601679.png" class title="image-20260213092601679">

<h2 id="一、README"><a href="#一、README" class="headerlink" title="一、README"></a>一、README</h2><blockquote>
<p>A GPT trained and run in <strong>243 lines of pure, dependency-free Python</strong>. No PyTorch. No NumPy. No nothing. Just <code>import math</code> and sheer will.</p>
<p>Based on <a class="link" target="_blank" rel="noopener" href="https://x.com/karpathy">Andrej Karpathy<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>‘s <a class="link" target="_blank" rel="noopener" href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95">microgpt<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> – the most distilled version of the GPT algorithm that can exist.</p>
<p>仅用 243 行纯 Python 代码（无任何依赖，仅导入 math）实现并运行的 GPT</p>
<p>基于 <a class="link" target="_blank" rel="noopener" href="https://x.com/karpathy">Andrej Karpathy<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>的<a class="link" target="_blank" rel="noopener" href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95">microgpt<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> – 最精简版本的GPT算法实现</p>
</blockquote>
<p>对Andrej Karpathy来说，这是一个“艺术项目”</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">The most atomic way to train and inference a GPT in pure, dependency-free Python.</span></span><br><span class="line"><span class="string">This file is the complete algorithm.</span></span><br><span class="line"><span class="string">Everything else is just efficiency.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@karpathy</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>至此，我们阅读这一repo的初衷应是：理解ChatGPT最本质、返璞归真的算法过程</p>
<p>项目组成非常的简单</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">╭─ /mnt/d/GithubRepos/karpathy-microgpt  on master wip ▓▒░─────────────────────────────────────────────────────────────────░▒▓ ✔  took 12s  base Py  at 09:40:05 </span><br><span class="line">╰─ tree .                             </span><br><span class="line">.</span><br><span class="line">├── README.md</span><br><span class="line">├── gpt.py</span><br><span class="line">├── input.txt</span><br><span class="line">├── model-sources</span><br><span class="line">│   ├── README.md</span><br><span class="line">│   ├── download_wikipedia.py</span><br><span class="line">│   ├── pyproject.toml</span><br><span class="line">│   └── uv.lock</span><br><span class="line">├── persistence</span><br><span class="line">│   ├── README.md</span><br><span class="line">│   ├── run.py</span><br><span class="line">│   ├── run.sh</span><br><span class="line">│   ├── train.py</span><br><span class="line">│   └── train.sh</span><br><span class="line">└── train.sh</span><br><span class="line"></span><br><span class="line">3 directories, 13 files</span><br></pre></td></tr></table></figure></div>

<p>为了更好的阅读代码，笔者将README中阐述的工作流放在这里，使得读者能够预先有一个宏观的认识：</p>
<div class="code-container" data-rel="Text"><figure class="iseeu highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. Load the dataset</span><br><span class="line">2. Build a tokenizer</span><br><span class="line">3. Implement autograd from scratch</span><br><span class="line">4. Initialize model parameters</span><br><span class="line">5. Define the forward pass</span><br><span class="line">6. Train with Adam</span><br><span class="line">7. Generate names</span><br></pre></td></tr></table></figure></div>

<h2 id="二、What-does-this-repo-have-done"><a href="#二、What-does-this-repo-have-done" class="headerlink" title="二、What does this repo have done?"></a>二、What does this repo have done?</h2><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os       <span class="comment"># os.path.exists</span></span><br><span class="line"><span class="keyword">import</span> math     <span class="comment"># math.log, math.exp</span></span><br><span class="line"><span class="keyword">import</span> random   <span class="comment"># random.seed, random.choices, random.gauss, random.shuffle</span></span><br><span class="line"><span class="comment"># Let there be order among chaos</span></span><br><span class="line">random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>这个简单的代码项目仅引入了<code>&quot;os&quot;&quot;math&quot;&quot;random&quot;</code>三个简单的库</li>
</ul>
<h3 id="Load-the-dataset"><a href="#Load-the-dataset" class="headerlink" title="Load the dataset"></a>Load the dataset</h3><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;input.txt&#x27;</span>):</span><br><span class="line">    <span class="keyword">import</span> urllib.request</span><br><span class="line">    names_url = <span class="string">&#x27;https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt&#x27;</span></span><br><span class="line">    urllib.request.urlretrieve(names_url, <span class="string">&#x27;input.txt&#x27;</span>)</span><br><span class="line">docs = [l.strip() <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&#x27;input.txt&#x27;</span>).read().strip().split(<span class="string">&#x27;\n&#x27;</span>) <span class="keyword">if</span> l.strip()] <span class="comment"># list[str] of documents</span></span><br><span class="line">random.shuffle(docs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;num docs: <span class="subst">&#123;<span class="built_in">len</span>(docs)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>打开目录下<code>input</code>文件（不存在则下载后打开）</li>
<li>将<code>input</code>文件内容转化为列表，剔除空行、删去首位空格保存为<code>docs</code>变量</li>
<li>随机打乱<code>docs</code>的顺序</li>
</ul>
<h3 id="Build-a-tokenizer"><a href="#Build-a-tokenizer" class="headerlink" title="Build a tokenizer"></a>Build a tokenizer</h3><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let there be a Tokenizer to translate strings to discrete symbols and back</span></span><br><span class="line">chars = [<span class="string">&#x27;&lt;BOS&gt;&#x27;</span>] + <span class="built_in">sorted</span>(<span class="built_in">set</span>(<span class="string">&#x27;&#x27;</span>.join(docs))) <span class="comment"># character-level tokenizer with a BOS delimiter</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(chars)</span><br><span class="line">stoi = &#123; ch:i <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125; <span class="comment"># encoding: map string to integer</span></span><br><span class="line">itos = &#123; i:ch <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125; <span class="comment"># decoding: map integer to string</span></span><br><span class="line">BOS = stoi[<span class="string">&#x27;&lt;BOS&gt;&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;vocab size: <span class="subst">&#123;vocab_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>将<code>docs</code>的内容按照单个字符拆解并排序，保存在以<code>&#39;&lt;BOS&gt;&#39;</code>为首元素的<code>chars</code>列表中</li>
<li><code>stoi</code>：将<code>chars</code>的元素与下标建立映射，也就是将字符转化为数的过程，字符→整数，即编码</li>
<li><code>itos</code>：逆过程，将数转化为<code>chars</code>对应字符元素的过程，整数→字符，即解码</li>
<li><code>BOS</code>：&lt;BOS&gt;是 “Beginning of Sequence” 的缩写，即 “序列开始” 标记。在语言模型中，它用来标识一个文本序列的开头。这里<code>BOS</code>的值为0</li>
</ul>
<p>总体来说，构建一个<strong>字符级别的 Tokenizer（分词器）</strong> —— 把字符串（姓名）转换成模型能理解的整数符号，同时也能把整数还原回字符串，还额外添加了&lt;BOS&gt;（句子开始）标记</p>
<h3 id="Implement-autograd-from-scratch"><a href="#Implement-autograd-from-scratch" class="headerlink" title="Implement autograd from scratch"></a>Implement autograd from scratch</h3><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let there be an Autograd to apply the chain rule recursively across a computation graph and so</span></span><br><span class="line"><span class="comment"># calculate the gradients of the loss with respect to model parameters.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Value</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Stores a single scalar value and its gradient.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, _children=(<span class="params"></span>), _op=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.grad = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>._backward = <span class="keyword">lambda</span>: <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>._prev = <span class="built_in">set</span>(_children)</span><br><span class="line">        <span class="variable language_">self</span>._op = _op <span class="comment"># the op that produced this node, for graphviz / debugging / etc</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line">        out = Value(<span class="variable language_">self</span>.data + other.data, (<span class="variable language_">self</span>, other), <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += out.grad</span><br><span class="line">            other.grad += out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self, other</span>):</span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line">        out = Value(<span class="variable language_">self</span>.data * other.data, (<span class="variable language_">self</span>, other), <span class="string">&#x27;*&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += other.data * out.grad</span><br><span class="line">            other.grad += <span class="variable language_">self</span>.data * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__pow__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(other, (<span class="built_in">int</span>, <span class="built_in">float</span>)), <span class="string">&quot;only supporting int/float powers for now&quot;</span></span><br><span class="line">        out = Value(<span class="variable language_">self</span>.data**other, (<span class="variable language_">self</span>,), <span class="string">f&#x27;**<span class="subst">&#123;other&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += (other * <span class="variable language_">self</span>.data**(other-<span class="number">1</span>)) * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self</span>):</span><br><span class="line">        out = Value(math.log(<span class="variable language_">self</span>.data), (<span class="variable language_">self</span>,), <span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += (<span class="number">1</span> / <span class="variable language_">self</span>.data) * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exp</span>(<span class="params">self</span>):</span><br><span class="line">        out = Value(math.exp(<span class="variable language_">self</span>.data), (<span class="variable language_">self</span>,), <span class="string">&#x27;exp&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += out.data * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self</span>):</span><br><span class="line">        out = Value(<span class="number">0</span> <span class="keyword">if</span> <span class="variable language_">self</span>.data &lt; <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.data, (<span class="variable language_">self</span>,), <span class="string">&#x27;ReLU&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += (out.data &gt; <span class="number">0</span>) * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># topological order all of the children in the graph</span></span><br><span class="line">        topo = []</span><br><span class="line">        visited = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">build_topo</span>(<span class="params">v</span>):</span><br><span class="line">            <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                visited.add(v)</span><br><span class="line">                <span class="keyword">for</span> child <span class="keyword">in</span> v._prev:</span><br><span class="line">                    build_topo(child)</span><br><span class="line">                topo.append(v)</span><br><span class="line">        build_topo(<span class="variable language_">self</span>)</span><br><span class="line">        <span class="comment"># go one variable at a time and apply the chain rule to get its gradient</span></span><br><span class="line">        <span class="variable language_">self</span>.grad = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">reversed</span>(topo):</span><br><span class="line">            v._backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__neg__</span>(<span class="params">self</span>): <span class="keyword">return</span> <span class="variable language_">self</span> * -<span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__radd__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> + other</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__sub__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> + (-other)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rsub__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> other + (-<span class="variable language_">self</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rmul__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> * other</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__truediv__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> * other**-<span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rtruediv__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> other * <span class="variable language_">self</span>**-<span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>): <span class="keyword">return</span> <span class="string">f&quot;Value(data=<span class="subst">&#123;self.data&#125;</span>, grad=<span class="subst">&#123;self.grad&#125;</span>)&quot;</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li><code>value</code>类包装了可变对象，重定义了<code>+、-、*、/、pow、log、e^x</code>等运算，同时为<code>out</code>节点定义<code>_backward</code>函数，使得后续梯度能反向传播给父节点</li>
<li><code>backword</code>方法通过依赖关系，拓扑排序得到所有的计算图节点顺序，保证计算某一结点时其依赖的所有节点信息均已计算完毕</li>
<li><code>__repr_</code>自定义了打印的格式，直观看到数据和梯度</li>
</ul>
<p>该类手动实现标量级的自动微分（反向传播），无需依赖任何框架，能自动计算损失函数对模型参数的梯度，支撑后续 GPT 模型的训练。</p>
<h4 id="基础属性：存储计算图与状态"><a href="#基础属性：存储计算图与状态" class="headerlink" title="基础属性：存储计算图与状态"></a>基础属性：存储计算图与状态</h4><p><code>__init__</code>方法定义了每个<code>Value</code>节点的核心属性，是构建计算图的基础：</p>
<table>
<thead>
<tr>
<th align="left">属性</th>
<th align="left">作用</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>self.data</code></td>
<td align="left">存储标量数值（如模型参数值、运算结果值）</td>
</tr>
<tr>
<td align="left"><code>self.grad</code></td>
<td align="left">存储梯度（初始为 0，反向传播时更新）</td>
</tr>
<tr>
<td align="left"><code>self._backward</code></td>
<td align="left">存储当前节点的反向传播逻辑（梯度传递规则），默认空函数</td>
</tr>
<tr>
<td align="left"><code>self._prev</code></td>
<td align="left">记录当前节点的父节点（依赖节点），构建计算图的核心</td>
</tr>
<tr>
<td align="left"><code>self._op</code></td>
<td align="left">记录生成当前节点的运算类型（如<code>+</code>&#x2F;<code>*</code>&#x2F;<code>log</code>），用于调试</td>
</tr>
</tbody></table>
<h4 id="正向运算：覆盖核心数学操作"><a href="#正向运算：覆盖核心数学操作" class="headerlink" title="正向运算：覆盖核心数学操作"></a>正向运算：覆盖核心数学操作</h4><p>通过重载运算符 &#x2F; 自定义方法，实现 GPT 所需的所有基础运算，且所有运算都遵循两个规则：</p>
<ul>
<li>类型统一：非<code>Value</code>类型（如 int&#x2F;float）自动转为<code>Value</code>实例；</li>
<li>构建计算图：生成新节点时，记录父节点和运算类型，并绑定反向传播逻辑。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">运算类型</th>
<th align="left">实现方式</th>
<th align="left">核心作用</th>
</tr>
</thead>
<tbody><tr>
<td align="left">加法 &#x2F; 乘法 &#x2F; 幂</td>
<td align="left">重载<code>__add__</code>&#x2F;<code>__mul__</code>&#x2F;<code>__pow__</code></td>
<td align="left">基础算术运算，是所有复杂运算的核心</td>
</tr>
<tr>
<td align="left">对数 &#x2F; 指数</td>
<td align="left">自定义<code>log()</code>&#x2F;<code>exp()</code></td>
<td align="left">支撑 softmax、交叉熵损失等 GPT 核心计算</td>
</tr>
<tr>
<td align="left">ReLU</td>
<td align="left">自定义<code>relu()</code></td>
<td align="left">实现神经网络的非线性激活</td>
</tr>
<tr>
<td align="left">减法 &#x2F; 除法 &#x2F; 负号</td>
<td align="left">重载<code>__sub__</code>&#x2F;<code>__truediv__</code>&#x2F;<code>__neg__</code></td>
<td align="left">基于加法 &#x2F; 乘法封装，减少代码冗余</td>
</tr>
<tr>
<td align="left">反向运算</td>
<td align="left">重载<code>__radd__</code>&#x2F;<code>__rmul__</code>等</td>
<td align="left">支持<code>2 + a</code>&#x2F;<code>3 * b</code>等自然写法，保证运算兼容性</td>
</tr>
</tbody></table>
<h4 id="反向传播：自动计算梯度"><a href="#反向传播：自动计算梯度" class="headerlink" title=". 反向传播：自动计算梯度"></a>. 反向传播：自动计算梯度</h4><p><code>backward()</code>方法实现梯度的自动计算，分两步保证正确性：</p>
<ol>
<li>拓扑排序（build_topo）：<ul>
<li>递归遍历计算图，按 “底层依赖在前、顶层损失在后” 的顺序排列节点；</li>
<li>避免梯度计算顺序错误（父节点未处理就处理子节点）。</li>
</ul>
</li>
<li>反向遍历 + 链式法则：<ul>
<li>根节点（损失）梯度初始化为 1（<code>dL/dL=1</code>），作为梯度传递起点；</li>
<li>反向遍历拓扑序，执行每个节点的<code>_backward()</code>，按预定义的梯度规则传递梯度；</li>
<li>所有梯度更新用<code>+=</code>，保证节点复用（一个节点参与多个运算时梯度累加）。</li>
</ul>
</li>
</ol>
<h4 id="关键特性与设计巧思"><a href="#关键特性与设计巧思" class="headerlink" title="关键特性与设计巧思"></a>关键特性与设计巧思</h4><ol>
<li><strong>闭包的运用</strong>：每个运算的<code>_backward</code>函数是嵌套函数，能 “记住” 外层的<code>self</code>&#x2F;<code>other</code>&#x2F;<code>out</code>，无需额外传参，简洁实现梯度规则；</li>
<li><strong>复用逻辑</strong>：减法 &#x2F; 除法等运算不单独定义梯度，而是转成加法 &#x2F; 乘法 &#x2F; 幂运算，复用已有梯度规则，减少冗余；</li>
<li><strong>可变对象特性</strong>：<code>Value</code>是可变对象，修改<code>grad</code>属性会直接作用于原始节点，保证梯度传递到模型参数本身；</li>
</ol>
<h3 id="Initialize-model-parameters"><a href="#Initialize-model-parameters" class="headerlink" title="Initialize model parameters"></a>Initialize model parameters</h3><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the parameters, to store the knowledge of the model.</span></span><br><span class="line">n_embd = <span class="number">16</span>     <span class="comment"># embedding dimension</span></span><br><span class="line">n_head = <span class="number">4</span>      <span class="comment"># number of attention heads</span></span><br><span class="line">n_layer = <span class="number">1</span>     <span class="comment"># number of layers</span></span><br><span class="line">block_size = <span class="number">8</span>  <span class="comment"># maximum sequence length</span></span><br><span class="line">head_dim = n_embd // n_head <span class="comment"># dimension of each head</span></span><br><span class="line">matrix = <span class="keyword">lambda</span> nout, nin, std=<span class="number">0.02</span>: [[Value(random.gauss(<span class="number">0</span>, std)) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nin)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nout)]</span><br><span class="line">state_dict = &#123;<span class="string">&#x27;wte&#x27;</span>: matrix(vocab_size, n_embd), <span class="string">&#x27;wpe&#x27;</span>: matrix(block_size, n_embd), <span class="string">&#x27;lm_head&#x27;</span>: matrix(vocab_size, n_embd)&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layer):</span><br><span class="line">    state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wq&#x27;</span>] = matrix(n_embd, n_embd)</span><br><span class="line">    state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wk&#x27;</span>] = matrix(n_embd, n_embd)</span><br><span class="line">    state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wv&#x27;</span>] = matrix(n_embd, n_embd)</span><br><span class="line">    state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wo&#x27;</span>] = matrix(n_embd, n_embd, std=<span class="number">0</span>)</span><br><span class="line">    state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.mlp_fc1&#x27;</span>] = matrix(<span class="number">4</span> * n_embd, n_embd)</span><br><span class="line">    state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.mlp_fc2&#x27;</span>] = matrix(n_embd, <span class="number">4</span> * n_embd, std=<span class="number">0</span>)</span><br><span class="line">params = [p <span class="keyword">for</span> mat <span class="keyword">in</span> state_dict.values() <span class="keyword">for</span> row <span class="keyword">in</span> mat <span class="keyword">for</span> p <span class="keyword">in</span> row] <span class="comment"># flatten params into a single list[Value]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;num params: <span class="subst">&#123;<span class="built_in">len</span>(params)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p>定义了超参数：</p>
<ul>
<li><code>n_embd = 16</code>：嵌入维度，每个字符转成16维向量</li>
<li><code>n_head = 4</code>：注意力头，把16维嵌入拆成4个4维的注意力头</li>
<li><code>n_layer = 1</code>：1层Transformer</li>
<li><code>block_size = 8</code>：模型最多处理8个字符的序列，如输入7个字符预测第8个</li>
<li><code>head_dim = n_embd</code>：每个注意力头的维度</li>
</ul>
</li>
<li><p>定义了权重矩阵初始化函数<code>matrix</code>，该匿名函数用于生成指定形状的权重矩阵，且每个元素都是<code>Value</code>实例</p>
<ul>
<li><code>nout</code>：矩阵行数（输出维度）</li>
<li><code>nin</code>：矩阵列数（输入维度）</li>
<li><code>std=0.02</code>：高斯分布标准差</li>
</ul>
</li>
<li><p>初始化模型所有权重矩阵（state_dict）</p>
<ul>
<li>注意力层：查询&#x2F;键&#x2F;值权重矩阵（n_embd×n_embd）</li>
<li>注意力输出投影矩阵（n_embd×n_embd），std&#x3D;0 → 初始值全0（稳定训练）</li>
<li>MLP层：第一层（4*n_embd×n_embd）→ 升维（GPT的MLP通常先升维4倍）</li>
<li>MLP层：第二层（n_embd×4*n_embd）→ 降维，std&#x3D;0 → 初始值全0</li>
<li><code>state_dict</code>（状态字典）是 GPT 存储所有参数的核心结构，键名对应模型组件，值是权重矩阵：</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left">权重键名</th>
<th align="left">对应组件</th>
<th align="left">形状</th>
<th align="left">作用</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>wte</code></td>
<td align="left">字符嵌入（Word Token Embedding）</td>
<td align="left">vocab_size × n_embd</td>
<td align="left">把字符（整数）转成 n_embd（16） 维向量</td>
</tr>
<tr>
<td align="left"><code>wpe</code></td>
<td align="left">位置嵌入（Word Position Embedding）</td>
<td align="left">block_size × n_embd</td>
<td align="left">把字符位置block_size（0~7）转成n_embd（16）维向量</td>
</tr>
<tr>
<td align="left"><code>attn_wq/wk/wv</code></td>
<td align="left">注意力层的 Q&#x2F;K&#x2F;V 矩阵</td>
<td align="left">n_embd × n_embd</td>
<td align="left">把嵌入向量转成查询 &#x2F; 键 &#x2F; 值向量</td>
</tr>
<tr>
<td align="left"><code>attn_wo</code></td>
<td align="left">注意力输出投影矩阵</td>
<td align="left">n_embd × n_embd</td>
<td align="left">把多头注意力的输出拼接后投影回 n_embd（16）维</td>
</tr>
<tr>
<td align="left"><code>mlp_fc1/fc2</code></td>
<td align="left">前馈网络（MLP）权重</td>
<td align="left">(4 × n_embd) × n_embd &#x2F; n_embd × (n_embd × 4)</td>
<td align="left">对注意力输出做非线性变换（升维→降维）</td>
</tr>
<tr>
<td align="left"><code>lm_head</code></td>
<td align="left">语言模型头</td>
<td align="left">vocab_size × n_embd</td>
<td align="left">把最终向量映射回词汇表，预测下一个字符</td>
</tr>
</tbody></table>
<ul>
<li>关键细节：<code>attn_wo</code>&#x2F;<code>mlp_fc2</code>的<code>std=0</code> → 初始值全为 0，目的是让模型训练初期更稳定（避免参数值过大）。</li>
</ul>
<h3 id="Define-the-forward-pass"><a href="#Define-the-forward-pass" class="headerlink" title="Define the forward pass"></a>Define the forward pass</h3><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.</span></span><br><span class="line"><span class="comment"># Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -&gt; rmsnorm, no biases, GeLU -&gt; ReLU^2</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x, w</span>):</span><br><span class="line">    <span class="keyword">return</span> [<span class="built_in">sum</span>(wi * xi <span class="keyword">for</span> wi, xi <span class="keyword">in</span> <span class="built_in">zip</span>(wo, x)) <span class="keyword">for</span> wo <span class="keyword">in</span> w]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">logits</span>):</span><br><span class="line">    max_val = <span class="built_in">max</span>(val.data <span class="keyword">for</span> val <span class="keyword">in</span> logits)</span><br><span class="line">    exps = [(val - max_val).exp() <span class="keyword">for</span> val <span class="keyword">in</span> logits]</span><br><span class="line">    total = <span class="built_in">sum</span>(exps)</span><br><span class="line">    <span class="keyword">return</span> [e / total <span class="keyword">for</span> e <span class="keyword">in</span> exps]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsnorm</span>(<span class="params">x</span>):</span><br><span class="line">    ms = <span class="built_in">sum</span>(xi * xi <span class="keyword">for</span> xi <span class="keyword">in</span> x) / <span class="built_in">len</span>(x)</span><br><span class="line">    scale = (ms + <span class="number">1e-5</span>) ** -<span class="number">0.5</span></span><br><span class="line">    <span class="keyword">return</span> [xi * scale <span class="keyword">for</span> xi <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt</span>(<span class="params">token_id, pos_id, keys, values</span>):</span><br><span class="line">    tok_emb = state_dict[<span class="string">&#x27;wte&#x27;</span>][token_id] <span class="comment"># token embedding</span></span><br><span class="line">    pos_emb = state_dict[<span class="string">&#x27;wpe&#x27;</span>][pos_id] <span class="comment"># position embedding</span></span><br><span class="line">    x = [t + p <span class="keyword">for</span> t, p <span class="keyword">in</span> <span class="built_in">zip</span>(tok_emb, pos_emb)] <span class="comment"># joint token and position embedding</span></span><br><span class="line">    x = rmsnorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> <span class="built_in">range</span>(n_layer):</span><br><span class="line">        <span class="comment"># 1) Multi-head attention block</span></span><br><span class="line">        x_residual = x</span><br><span class="line">        x = rmsnorm(x)</span><br><span class="line">        q = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wq&#x27;</span>])</span><br><span class="line">        k = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wk&#x27;</span>])</span><br><span class="line">        v = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wv&#x27;</span>])</span><br><span class="line">        keys[li].append(k)</span><br><span class="line">        values[li].append(v)</span><br><span class="line">        x_attn = []</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_head):</span><br><span class="line">            hs = h * head_dim</span><br><span class="line">            q_h = q[hs:hs+head_dim]</span><br><span class="line">            k_h = [ki[hs:hs+head_dim] <span class="keyword">for</span> ki <span class="keyword">in</span> keys[li]]</span><br><span class="line">            v_h = [vi[hs:hs+head_dim] <span class="keyword">for</span> vi <span class="keyword">in</span> values[li]]</span><br><span class="line">            attn_logits = [<span class="built_in">sum</span>(q_h[j] * k_h[t][j] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(head_dim)) / head_dim**<span class="number">0.5</span> <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(k_h))]</span><br><span class="line">            attn_weights = softmax(attn_logits)</span><br><span class="line">            head_out = [<span class="built_in">sum</span>(attn_weights[t] * v_h[t][j] <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v_h))) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(head_dim)]</span><br><span class="line">            x_attn.extend(head_out)</span><br><span class="line">        x = linear(x_attn, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wo&#x27;</span>])</span><br><span class="line">        x = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(x, x_residual)]</span><br><span class="line">        <span class="comment"># 2) MLP block</span></span><br><span class="line">        x_residual = x</span><br><span class="line">        x = rmsnorm(x)</span><br><span class="line">        x = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.mlp_fc1&#x27;</span>])</span><br><span class="line">        x = [xi.relu() ** <span class="number">2</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x]</span><br><span class="line">        x = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.mlp_fc2&#x27;</span>])</span><br><span class="line">        x = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(x, x_residual)]</span><br><span class="line"></span><br><span class="line">    logits = linear(x, state_dict[<span class="string">&#x27;lm_head&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure></div>

<ul>
<li>linear &#x3D; 向量 × 权重矩阵 &#x3D; 新向量</li>
<li>softmax &#x3D; 把分数变成归一化概率</li>
<li>rmsnorm &#x3D; 缩放向量，把向量标准化，使其长度稳定</li>
</ul>
<h4 id="gpt-嵌入部分"><a href="#gpt-嵌入部分" class="headerlink" title="gpt-嵌入部分"></a>gpt-嵌入部分</h4><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tok_emb = state_dict[<span class="string">&#x27;wte&#x27;</span>][token_id]</span><br><span class="line">pos_emb = state_dict[<span class="string">&#x27;wpe&#x27;</span>][pos_id]</span><br><span class="line">x = [t + p <span class="keyword">for</span> t, p <span class="keyword">in</span> <span class="built_in">zip</span>(tok_emb, pos_emb)]</span><br><span class="line">x = rmsnorm(x)</span><br></pre></td></tr></table></figure></div>

<p>将字 + 位置信息转换成向量，将文字转换成机器可理解的向量表示</p>
<p>rmsnorm(x)进行均方根归一化，让向量的 “幅值稳定”（避免某些维度数值过大 &#x2F; 过小）；和原版GPT-2 LayerNorm 的区别：RMSNorm 去掉了 “减均值” 和 “加偏置 &#x2F; 移位” 的步骤，只保留 “缩放”，实现更简单，且不需要偏置参数（代码里所有矩阵都没有偏置）；让后续的注意力 &#x2F; MLP 计算更稳定，避免训练时梯度爆炸 &#x2F; 消失。</p>
<h4 id="生成QKV"><a href="#生成QKV" class="headerlink" title="生成QKV"></a>生成QKV</h4><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Q/K/V投影</span></span><br><span class="line">q = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wq&#x27;</span>])</span><br><span class="line">k = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wk&#x27;</span>])</span><br><span class="line">v = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wv&#x27;</span>])</span><br><span class="line">keys[li].append(k)  <span class="comment"># KV缓存：存储历史K</span></span><br><span class="line">values[li].append(v)<span class="comment"># KV缓存：存储历史V</span></span><br></pre></td></tr></table></figure></div>

<p>每个字生成 3 个向量：</p>
<ul>
<li>Q：我在找什么</li>
<li>K：我有什么信息</li>
<li>V：我实际的内容</li>
</ul>
<p>QKV &#x3D; 让每个字学会 “查询、匹配、提供信息”</p>
<h4 id="多头注意力核心"><a href="#多头注意力核心" class="headerlink" title="多头注意力核心"></a>多头注意力核心</h4><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4个头分别计算注意力</span></span><br><span class="line"><span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_head):</span><br><span class="line">    q_h = q[hs:hs+head_dim]  <span class="comment"># 拆分当前头的Q</span></span><br><span class="line">    k_h = [ki[hs:hs+head_dim] <span class="keyword">for</span> ki <span class="keyword">in</span> keys[li]]  <span class="comment"># 历史K的当前头部分</span></span><br><span class="line">    v_h = [vi[hs:hs+head_dim] <span class="keyword">for</span> vi <span class="keyword">in</span> values[li]]  <span class="comment"># 历史V的当前头部分</span></span><br><span class="line">    <span class="comment"># 点积注意力分数</span></span><br><span class="line">    attn_logits = [<span class="built_in">sum</span>(q_h[j] * k_h[t][j] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(head_dim)) / head_dim**<span class="number">0.5</span> <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(k_h))]</span><br><span class="line">    <span class="comment"># softmax转权重</span></span><br><span class="line">    attn_weights = softmax(attn_logits)</span><br><span class="line">    <span class="comment"># 加权求和V</span></span><br><span class="line">    head_out = [<span class="built_in">sum</span>(attn_weights[t] * v_h[t][j] <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v_h))) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(head_dim)]</span><br><span class="line">    x_attn.extend(head_out)</span><br></pre></td></tr></table></figure></div>

<p>让当前字<strong>看前面所有字</strong>，算出 “我该关注谁”。</p>
<ul>
<li>Q&#x2F;K&#x2F;V 投影：用 3 个权重矩阵（wq&#x2F;wk&#x2F;wv）把当前向量转成 3 个新向量（Q &#x3D; 查询、K &#x3D; 键、V &#x3D; 值）；</li>
<li>KV 缓存（KV cache）：把当前字符的 K&#x2F;V 存入keys&#x2F;values列表，后续处理下一个字符时，直接复用历史 K&#x2F;V，不用重新计算 —— 这是提升推理效率的关键（原版 GPT-2 也用 KV 缓存）；</li>
<li>多头拆分计算：把 16 维的 Q&#x2F;K&#x2F;V 拆成 4 个 4 维的 “注意力头”，每个头独立计算注意力（相当于模型从 4 个不同角度关注上下文）；</li>
<li>点积注意力 + 加权求和：每个头计算 “当前字符该关注历史哪些字符” 的分数（点积）→ 转成概率（softmax）→ 用概率加权求和历史 V 向量，得到该头的输出；</li>
<li>最终把 4 个头的输出拼接，得到完整的注意力结果。</li>
</ul>
<p>注意力 &#x3D; 让模型学会看上下文</p>
<h4 id="残差连接（注意力层）"><a href="#残差连接（注意力层）" class="headerlink" title="残差连接（注意力层）"></a>残差连接（注意力层）</h4><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_residual = x  <span class="comment"># 保存注意力层的输入（残差）</span></span><br><span class="line"><span class="comment"># 注意力计算...</span></span><br><span class="line">x = linear(x_attn, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wo&#x27;</span>])</span><br><span class="line">x = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(x, x_residual)]  <span class="comment"># 注意力输出 + 原始输入</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>先把注意力层的输入向量保存为x_residual（残差）；</li>
<li>注意力计算完成后，把 “注意力输出” 和 “原始输入” 逐元素相加；</li>
<li>作用：解决深层网络的 “梯度消失” 问题 —— 即使网络层数多，信息也能直接从输入传递到输出，原版 GPT-2 的核心设计之一。</li>
</ul>
<h4 id="MLP-前馈网络"><a href="#MLP-前馈网络" class="headerlink" title="MLP 前馈网络"></a>MLP 前馈网络</h4><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.mlp_fc1&#x27;</span>])  <span class="comment"># 第一个线性层（升维）</span></span><br><span class="line">x = [xi.relu()**<span class="number">2</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x]  <span class="comment"># ReLU²激活</span></span><br><span class="line">x = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.mlp_fc2&#x27;</span>])  <span class="comment"># 第二个线性层（降维）</span></span><br></pre></td></tr></table></figure></div>

<p>在注意力之后，做<strong>非线性加工</strong>，让模型学会复杂逻辑。</p>
<ul>
<li>第一步：用 fc1 矩阵把 16 维向量升维到 64 维（扩大模型的 “思考空间”）；</li>
<li>第二步：用ReLU(x)²（ReLU 平方）做非线性激活 —— 原版 GPT-2 用 GeLU 激活，这里简化为 ReLU²，实现更简单，效果接近；</li>
<li>第三步：用 fc2 矩阵把 64 维向量降维回 16 维（回到原维度，方便后续残差连接）；</li>
<li>作用：对注意力层的结果做 “非线性加工”，让模型能学习更复杂的语义规律（比如从 “学编程”“学 GPT” 中总结出 “学习技术” 的逻辑）。</li>
</ul>
<h4 id="残差连接（MLP-层）"><a href="#残差连接（MLP-层）" class="headerlink" title="残差连接（MLP 层）"></a>残差连接（MLP 层）</h4><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_residual = x  <span class="comment"># 保存MLP层的输入（残差）</span></span><br><span class="line"><span class="comment"># MLP计算...</span></span><br><span class="line">x = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(x, x_residual)]  <span class="comment"># MLP输出 + 原始输入</span></span><br></pre></td></tr></table></figure></div>

<p>把 MLP 的输出和输入相加，进一步保证信息的传递。和注意力层的残差连接逻辑完全一致。</p>
<h4 id="logits-输出投影"><a href="#logits-输出投影" class="headerlink" title="logits 输出投影"></a>logits 输出投影</h4><div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits = linear(x, state_dict[<span class="string">&#x27;lm_head&#x27;</span>])  <span class="comment"># 映射到词汇表维度</span></span><br></pre></td></tr></table></figure></div>

<ul>
<li>用 lm_head 矩阵把最终的 16 维向量，映射到 “词汇表大小” 的维度（比如词汇表有 27 个字符，就输出 27 维）；</li>
<li>输出的每个维度对应 “下一个字符是该词汇的分数（logits）”，后续用 softmax 就能转成概率，预测下一个字符。</li>
</ul>
<p>logits &#x3D; 下一个字的预测分数</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>gpt()函数实现了GPT-2 变体的 “单字符步推理”（即每次只处理一个字符，而非整段序列）。包含至少3 处差异，对应代码的简化设计：</p>
<table>
<thead>
<tr>
<th align="left">差异点</th>
<th align="left">原版 GPT-2</th>
<th align="left">本实现（代码）</th>
<th align="left">简化目的</th>
</tr>
</thead>
<tbody><tr>
<td align="left">归一化方式</td>
<td align="left">LayerNorm（带偏置 &#x2F; 移位）</td>
<td align="left">RMSNorm（无偏置 &#x2F; 移位）</td>
<td align="left">实现更简单，减少参数</td>
</tr>
<tr>
<td align="left">偏置参数</td>
<td align="left">所有线性层都有偏置</td>
<td align="left">无任何偏置参数</td>
<td align="left">减少参数数量，简化计算</td>
</tr>
<tr>
<td align="left">激活函数</td>
<td align="left">GeLU（平滑非线性）</td>
<td align="left">ReLU²（ReLU 平方）</td>
<td align="left">实现更简单，无需复杂的 GeLU 计算</td>
</tr>
</tbody></table>
<ol>
<li>函数是 “单字符步” 推理（每次处理一个字符），7 个步骤完整覆盖 GPT-2 的核心流程；</li>
<li>多头注意力 + KV 缓存 + 残差连接是 GPT-2 的核心设计，本实现完全保留；</li>
<li>仅做了 3 处简化（RMSNorm &#x2F; 无偏置 &#x2F; ReLU²），目的是降低实现复杂度，核心逻辑和原版一致；</li>
<li>所有步骤都对应代码中的具体实现，从嵌入到输出的信息流动完全对齐。</li>
<li>简单说：这个gpt()函数是 “简化版 GPT-2”—— 保留了核心的注意力 &#x2F; 残差 &#x2F; 缓存逻辑，只做了非核心的简化</li>
</ol>
<h2 id="三、How-to-run-microgpt"><a href="#三、How-to-run-microgpt" class="headerlink" title="三、How to run microgpt?"></a>三、How to run microgpt?</h2><p>一行代码即可，逻辑是（如果没有则下载uv）使用nv启动环境，执行gpt.py代码文件</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./train.sh</span><br></pre></td></tr></table></figure></div>

<p>为了能让模型保存下来重用避免反复训练，我让豆包老师略微修改了代码，增添了保存模型&#x2F;加载模型的模块</p>
<img lazyload src="/images/loading.svg" data-src="/2026/02/13/Github-Repos-microgpt/Github-Repos-microgpt" alt="image-20260214090905724" style="zoom:150%;">

<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">The most atomic way to train and inference a GPT in pure, dependency-free Python.</span></span><br><span class="line"><span class="string">This file is the complete algorithm.</span></span><br><span class="line"><span class="string">Everything else is just efficiency.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@karpathy</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os       <span class="comment"># os.path.exists</span></span><br><span class="line"><span class="keyword">import</span> math     <span class="comment"># math.log, math.exp</span></span><br><span class="line"><span class="keyword">import</span> random   <span class="comment"># random.seed, random.choices, random.gauss, random.shuffle</span></span><br><span class="line"><span class="keyword">import</span> pickle   <span class="comment"># 用于模型序列化保存/加载（Python内置，无额外依赖）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 全局控制变量（核心开关） =====================</span></span><br><span class="line"><span class="comment"># 一键控制：True=加载已有模型（跳过训练），False=从头训练并保存模型</span></span><br><span class="line">LOAD_MODEL = <span class="literal">False</span>  </span><br><span class="line"><span class="comment"># 模型文件路径（加载/保存都用这个路径）</span></span><br><span class="line">MODEL_PATH = <span class="string">&quot;gpt_trained_model.pkl&quot;</span></span><br><span class="line"><span class="comment"># 训练相关参数（也可放这里统一控制）</span></span><br><span class="line">NUM_TRAIN_STEPS = <span class="number">500</span>  <span class="comment"># 训练步数</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-2</span>    <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Let there be order among chaos</span></span><br><span class="line">random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 1. 数据加载与Tokenizer初始化 =====================</span></span><br><span class="line"><span class="comment"># Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;input.txt&#x27;</span>):</span><br><span class="line">    <span class="keyword">import</span> urllib.request</span><br><span class="line">    names_url = <span class="string">&#x27;https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt&#x27;</span></span><br><span class="line">    urllib.request.urlretrieve(names_url, <span class="string">&#x27;input.txt&#x27;</span>)</span><br><span class="line">docs = [l.strip() <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&#x27;input.txt&#x27;</span>).read().strip().split(<span class="string">&#x27;\n&#x27;</span>) <span class="keyword">if</span> l.strip()] <span class="comment"># list[str] of documents</span></span><br><span class="line">random.shuffle(docs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;num docs: <span class="subst">&#123;<span class="built_in">len</span>(docs)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let there be a Tokenizer to translate strings to discrete symbols and back</span></span><br><span class="line">chars = [<span class="string">&#x27;&lt;BOS&gt;&#x27;</span>] + <span class="built_in">sorted</span>(<span class="built_in">set</span>(<span class="string">&#x27;&#x27;</span>.join(docs))) <span class="comment"># character-level tokenizer with a BOS delimiter</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(chars)</span><br><span class="line">stoi = &#123; ch:i <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125; <span class="comment"># encoding: map string to integer</span></span><br><span class="line">itos = &#123; i:ch <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125; <span class="comment"># decoding: map integer to string</span></span><br><span class="line">BOS = stoi[<span class="string">&#x27;&lt;BOS&gt;&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;vocab size: <span class="subst">&#123;vocab_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 2. 自动微分核心（Value类） =====================</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Value</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Stores a single scalar value and its gradient.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, _children=(<span class="params"></span>), _op=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        <span class="variable language_">self</span>.grad = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>._backward = <span class="keyword">lambda</span>: <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>._prev = <span class="built_in">set</span>(_children)</span><br><span class="line">        <span class="variable language_">self</span>._op = _op <span class="comment"># the op that produced this node, for graphviz / debugging / etc</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line">        out = Value(<span class="variable language_">self</span>.data + other.data, (<span class="variable language_">self</span>, other), <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += out.grad</span><br><span class="line">            other.grad += out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self, other</span>):</span><br><span class="line">        other = other <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, Value) <span class="keyword">else</span> Value(other)</span><br><span class="line">        out = Value(<span class="variable language_">self</span>.data * other.data, (<span class="variable language_">self</span>, other), <span class="string">&#x27;*&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += other.data * out.grad</span><br><span class="line">            other.grad += <span class="variable language_">self</span>.data * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__pow__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(other, (<span class="built_in">int</span>, <span class="built_in">float</span>)), <span class="string">&quot;only supporting int/float powers for now&quot;</span></span><br><span class="line">        out = Value(<span class="variable language_">self</span>.data**other, (<span class="variable language_">self</span>,), <span class="string">f&#x27;**<span class="subst">&#123;other&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += (other * <span class="variable language_">self</span>.data**(other-<span class="number">1</span>)) * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self</span>):</span><br><span class="line">        out = Value(math.log(<span class="variable language_">self</span>.data), (<span class="variable language_">self</span>,), <span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += (<span class="number">1</span> / <span class="variable language_">self</span>.data) * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exp</span>(<span class="params">self</span>):</span><br><span class="line">        out = Value(math.exp(<span class="variable language_">self</span>.data), (<span class="variable language_">self</span>,), <span class="string">&#x27;exp&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += out.data * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self</span>):</span><br><span class="line">        out = Value(<span class="number">0</span> <span class="keyword">if</span> <span class="variable language_">self</span>.data &lt; <span class="number">0</span> <span class="keyword">else</span> <span class="variable language_">self</span>.data, (<span class="variable language_">self</span>,), <span class="string">&#x27;ReLU&#x27;</span>)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_backward</span>():</span><br><span class="line">            <span class="variable language_">self</span>.grad += (out.data &gt; <span class="number">0</span>) * out.grad</span><br><span class="line">        out._backward = _backward</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># topological order all of the children in the graph</span></span><br><span class="line">        topo = []</span><br><span class="line">        visited = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">build_topo</span>(<span class="params">v</span>):</span><br><span class="line">            <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                visited.add(v)</span><br><span class="line">                <span class="keyword">for</span> child <span class="keyword">in</span> v._prev:</span><br><span class="line">                    build_topo(child)</span><br><span class="line">                topo.append(v)</span><br><span class="line">        build_topo(<span class="variable language_">self</span>)</span><br><span class="line">        <span class="comment"># go one variable at a time and apply the chain rule to get its gradient</span></span><br><span class="line">        <span class="variable language_">self</span>.grad = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">reversed</span>(topo):</span><br><span class="line">            v._backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__neg__</span>(<span class="params">self</span>): <span class="keyword">return</span> <span class="variable language_">self</span> * -<span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__radd__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> + other</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__sub__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> + (-other)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rsub__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> other + (-<span class="variable language_">self</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rmul__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> * other</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__truediv__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> <span class="variable language_">self</span> * other**-<span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rtruediv__</span>(<span class="params">self, other</span>): <span class="keyword">return</span> other * <span class="variable language_">self</span>**-<span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>): <span class="keyword">return</span> <span class="string">f&quot;Value(data=<span class="subst">&#123;self.data&#125;</span>, grad=<span class="subst">&#123;self.grad&#125;</span>)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 3. 模型保存/加载工具函数 =====================</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">state_dict, save_path=MODEL_PATH</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;保存模型参数到文件（仅保存data值，梯度无需保存）&quot;&quot;&quot;</span></span><br><span class="line">    save_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key, mat <span class="keyword">in</span> state_dict.items():</span><br><span class="line">        save_dict[key] = [[v.data <span class="keyword">for</span> v <span class="keyword">in</span> row] <span class="keyword">for</span> row <span class="keyword">in</span> mat]</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(save_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(save_dict, f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n✅ 模型已保存到: <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">load_path=MODEL_PATH</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从文件加载模型参数，恢复为Value实例的state_dict&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(load_path):</span><br><span class="line">        <span class="keyword">raise</span> FileNotFoundError(<span class="string">f&quot;\n❌ 模型文件不存在: <span class="subst">&#123;load_path&#125;</span>（请先设置LOAD_MODEL=False训练模型）&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(load_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        save_dict = pickle.load(f)</span><br><span class="line">    </span><br><span class="line">    state_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key, mat <span class="keyword">in</span> save_dict.items():</span><br><span class="line">        state_dict[key] = [[Value(v) <span class="keyword">for</span> v <span class="keyword">in</span> row] <span class="keyword">for</span> row <span class="keyword">in</span> mat]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n✅ 模型已从: <span class="subst">&#123;load_path&#125;</span> 加载&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> state_dict</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 4. 模型超参数与初始化 =====================</span></span><br><span class="line">n_embd = <span class="number">16</span>     <span class="comment"># embedding dimension</span></span><br><span class="line">n_head = <span class="number">4</span>      <span class="comment"># number of attention heads</span></span><br><span class="line">n_layer = <span class="number">1</span>     <span class="comment"># number of layers</span></span><br><span class="line">block_size = <span class="number">8</span>  <span class="comment"># maximum sequence length</span></span><br><span class="line">head_dim = n_embd // n_head <span class="comment"># dimension of each head</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重矩阵初始化函数</span></span><br><span class="line">matrix = <span class="keyword">lambda</span> nout, nin, std=<span class="number">0.02</span>: [[Value(random.gauss(<span class="number">0</span>, std)) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nin)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nout)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化/加载state_dict</span></span><br><span class="line">state_dict = <span class="literal">None</span></span><br><span class="line">params = []  <span class="comment"># 模型参数扁平列表（全局可用）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> LOAD_MODEL:</span><br><span class="line">    <span class="comment"># 分支1：加载已有模型（跳过训练）</span></span><br><span class="line">    state_dict = load_model()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 分支2：从头初始化模型（准备训练）</span></span><br><span class="line">    state_dict = &#123;</span><br><span class="line">        <span class="string">&#x27;wte&#x27;</span>: matrix(vocab_size, n_embd),</span><br><span class="line">        <span class="string">&#x27;wpe&#x27;</span>: matrix(block_size, n_embd),</span><br><span class="line">        <span class="string">&#x27;lm_head&#x27;</span>: matrix(vocab_size, n_embd)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layer):</span><br><span class="line">        state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wq&#x27;</span>] = matrix(n_embd, n_embd)</span><br><span class="line">        state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wk&#x27;</span>] = matrix(n_embd, n_embd)</span><br><span class="line">        state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wv&#x27;</span>] = matrix(n_embd, n_embd)</span><br><span class="line">        state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.attn_wo&#x27;</span>] = matrix(n_embd, n_embd, std=<span class="number">0</span>)</span><br><span class="line">        state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.mlp_fc1&#x27;</span>] = matrix(<span class="number">4</span> * n_embd, n_embd)</span><br><span class="line">        state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;i&#125;</span>.mlp_fc2&#x27;</span>] = matrix(n_embd, <span class="number">4</span> * n_embd, std=<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;✅ 模型参数初始化完成&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成参数扁平列表（无论加载/初始化都要执行）</span></span><br><span class="line">params = [p <span class="keyword">for</span> mat <span class="keyword">in</span> state_dict.values() <span class="keyword">for</span> row <span class="keyword">in</span> mat <span class="keyword">for</span> p <span class="keyword">in</span> row]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;✅ 模型参数总数: <span class="subst">&#123;<span class="built_in">len</span>(params)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 5. 模型前向传播逻辑 =====================</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x, w</span>):</span><br><span class="line">    <span class="keyword">return</span> [<span class="built_in">sum</span>(wi * xi <span class="keyword">for</span> wi, xi <span class="keyword">in</span> <span class="built_in">zip</span>(wo, x)) <span class="keyword">for</span> wo <span class="keyword">in</span> w]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">logits</span>):</span><br><span class="line">    max_val = <span class="built_in">max</span>(val.data <span class="keyword">for</span> val <span class="keyword">in</span> logits)</span><br><span class="line">    exps = [(val - max_val).exp() <span class="keyword">for</span> val <span class="keyword">in</span> logits]</span><br><span class="line">    total = <span class="built_in">sum</span>(exps)</span><br><span class="line">    <span class="keyword">return</span> [e / total <span class="keyword">for</span> e <span class="keyword">in</span> exps]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsnorm</span>(<span class="params">x</span>):</span><br><span class="line">    ms = <span class="built_in">sum</span>(xi * xi <span class="keyword">for</span> xi <span class="keyword">in</span> x) / <span class="built_in">len</span>(x)</span><br><span class="line">    scale = (ms + <span class="number">1e-5</span>) ** -<span class="number">0.5</span></span><br><span class="line">    <span class="keyword">return</span> [xi * scale <span class="keyword">for</span> xi <span class="keyword">in</span> x]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt</span>(<span class="params">token_id, pos_id, keys, values</span>):</span><br><span class="line">    tok_emb = state_dict[<span class="string">&#x27;wte&#x27;</span>][token_id] <span class="comment"># token embedding</span></span><br><span class="line">    pos_emb = state_dict[<span class="string">&#x27;wpe&#x27;</span>][pos_id] <span class="comment"># position embedding</span></span><br><span class="line">    x = [t + p <span class="keyword">for</span> t, p <span class="keyword">in</span> <span class="built_in">zip</span>(tok_emb, pos_emb)] <span class="comment"># joint token and position embedding</span></span><br><span class="line">    x = rmsnorm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> <span class="built_in">range</span>(n_layer):</span><br><span class="line">        <span class="comment"># 1) Multi-head attention block</span></span><br><span class="line">        x_residual = x</span><br><span class="line">        x = rmsnorm(x)</span><br><span class="line">        q = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wq&#x27;</span>])</span><br><span class="line">        k = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wk&#x27;</span>])</span><br><span class="line">        v = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wv&#x27;</span>])</span><br><span class="line">        keys[li].append(k)</span><br><span class="line">        values[li].append(v)</span><br><span class="line">        x_attn = []</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_head):</span><br><span class="line">            hs = h * head_dim</span><br><span class="line">            q_h = q[hs:hs+head_dim]</span><br><span class="line">            k_h = [ki[hs:hs+head_dim] <span class="keyword">for</span> ki <span class="keyword">in</span> keys[li]]</span><br><span class="line">            v_h = [vi[hs:hs+head_dim] <span class="keyword">for</span> vi <span class="keyword">in</span> values[li]]</span><br><span class="line">            attn_logits = [<span class="built_in">sum</span>(q_h[j] * k_h[t][j] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(head_dim)) / head_dim**<span class="number">0.5</span> <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(k_h))]</span><br><span class="line">            attn_weights = softmax(attn_logits)</span><br><span class="line">            head_out = [<span class="built_in">sum</span>(attn_weights[t] * v_h[t][j] <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v_h))) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(head_dim)]</span><br><span class="line">            x_attn.extend(head_out)</span><br><span class="line">        x = linear(x_attn, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.attn_wo&#x27;</span>])</span><br><span class="line">        x = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(x, x_residual)]</span><br><span class="line">        <span class="comment"># 2) MLP block</span></span><br><span class="line">        x_residual = x</span><br><span class="line">        x = rmsnorm(x)</span><br><span class="line">        x = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.mlp_fc1&#x27;</span>])</span><br><span class="line">        x = [xi.relu() ** <span class="number">2</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x]</span><br><span class="line">        x = linear(x, state_dict[<span class="string">f&#x27;layer<span class="subst">&#123;li&#125;</span>.mlp_fc2&#x27;</span>])</span><br><span class="line">        x = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(x, x_residual)]</span><br><span class="line"></span><br><span class="line">    logits = linear(x, state_dict[<span class="string">&#x27;lm_head&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 6. 训练逻辑（仅当LOAD_MODEL=False时执行） =====================</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> LOAD_MODEL:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n--- 开始训练 ---&quot;</span>)</span><br><span class="line">    <span class="comment"># Adam优化器初始化</span></span><br><span class="line">    beta1, beta2, eps_adam = <span class="number">0.9</span>, <span class="number">0.95</span>, <span class="number">1e-8</span></span><br><span class="line">    m = [<span class="number">0.0</span>] * <span class="built_in">len</span>(params) <span class="comment"># first moment buffer</span></span><br><span class="line">    v = [<span class="number">0.0</span>] * <span class="built_in">len</span>(params) <span class="comment"># second moment buffer</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(NUM_TRAIN_STEPS):</span><br><span class="line">        <span class="comment"># 采样一个文档并分词</span></span><br><span class="line">        doc = docs[step % <span class="built_in">len</span>(docs)]</span><br><span class="line">        tokens = [BOS] + [stoi[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> doc] + [BOS]</span><br><span class="line">        n = <span class="built_in">min</span>(block_size, <span class="built_in">len</span>(tokens) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播计算损失</span></span><br><span class="line">        keys, values = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer)], [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer)]</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> pos_id <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            token_id, target_id = tokens[pos_id], tokens[pos_id + <span class="number">1</span>]</span><br><span class="line">            logits = gpt(token_id, pos_id, keys, values)</span><br><span class="line">            probs = softmax(logits)</span><br><span class="line">            loss_t = -probs[target_id].log()</span><br><span class="line">            losses.append(loss_t)</span><br><span class="line">        loss = (<span class="number">1</span> / n) * <span class="built_in">sum</span>(losses)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Adam优化器更新参数</span></span><br><span class="line">        lr_t = LEARNING_RATE * (<span class="number">1</span> - step / NUM_TRAIN_STEPS) <span class="comment"># 学习率衰减</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(params):</span><br><span class="line">            m[i] = beta1 * m[i] + (<span class="number">1</span> - beta1) * p.grad</span><br><span class="line">            v[i] = beta2 * v[i] + (<span class="number">1</span> - beta2) * p.grad ** <span class="number">2</span></span><br><span class="line">            m_hat = m[i] / (<span class="number">1</span> - beta1 ** (step + <span class="number">1</span>))</span><br><span class="line">            v_hat = v[i] / (<span class="number">1</span> - beta2 ** (step + <span class="number">1</span>))</span><br><span class="line">            p.data -= lr_t * m_hat / (v_hat ** <span class="number">0.5</span> + eps_adam)</span><br><span class="line">            p.grad = <span class="number">0</span> <span class="comment"># 梯度清零</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印训练进度</span></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;step <span class="subst">&#123;step+<span class="number">1</span>:4d&#125;</span> / <span class="subst">&#123;NUM_TRAIN_STEPS:4d&#125;</span> | loss <span class="subst">&#123;loss.data:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练结束后保存模型</span></span><br><span class="line">    save_model(state_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ===================== 7. 推理逻辑（无论加载/训练都执行） =====================</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- 开始推理生成文本 ---&quot;</span>)</span><br><span class="line">temperature = <span class="number">0.6</span> <span class="comment"># 控制生成随机性（越小越确定，越大越随机）</span></span><br><span class="line"><span class="keyword">for</span> sample_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    keys, values = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer)], [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer)]</span><br><span class="line">    token_id = BOS</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;样本 <span class="subst">&#123;sample_idx+<span class="number">1</span>:2d&#125;</span>: &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> pos_id <span class="keyword">in</span> <span class="built_in">range</span>(block_size):</span><br><span class="line">        <span class="comment"># 前向传播获取预测概率</span></span><br><span class="line">        logits = gpt(token_id, pos_id, keys, values)</span><br><span class="line">        <span class="comment"># 温度缩放 + softmax转概率</span></span><br><span class="line">        probs = softmax([l / temperature <span class="keyword">for</span> l <span class="keyword">in</span> logits])</span><br><span class="line">        <span class="comment"># 按概率采样下一个token</span></span><br><span class="line">        token_id = random.choices(<span class="built_in">range</span>(vocab_size), weights=[p.data <span class="keyword">for</span> p <span class="keyword">in</span> probs])[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 遇到BOS则停止生成</span></span><br><span class="line">        <span class="keyword">if</span> token_id == BOS:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 打印生成的字符</span></span><br><span class="line">        <span class="built_in">print</span>(itos[token_id], end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure></div>

<div class="code-container" data-rel="Text"><figure class="iseeu highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">╭─ /mnt/d/GithubRepos/karpathy-microgpt  on master wip !1 ▓▒░──────────────────────────────────────────────────░▒▓ ✔  took 1m 31s  base Py  at 09:04:18 </span><br><span class="line">╰─ ./train.sh         </span><br><span class="line">num docs: 32033</span><br><span class="line">vocab size: 27</span><br><span class="line">✅ 模型参数初始化完成</span><br><span class="line">✅ 模型参数总数: 4064</span><br><span class="line"></span><br><span class="line">--- 开始训练 ---</span><br><span class="line">step    1 /  500 | loss 3.2627</span><br><span class="line">step   10 /  500 | loss 3.2242</span><br><span class="line">step   20 /  500 | loss 2.4210</span><br><span class="line">step   30 /  500 | loss 2.3740</span><br><span class="line">step   40 /  500 | loss 2.9382</span><br><span class="line">step   50 /  500 | loss 2.3823</span><br><span class="line">step   60 /  500 | loss 3.3081</span><br><span class="line">step   70 /  500 | loss 3.1322</span><br><span class="line">step   80 /  500 | loss 2.4062</span><br><span class="line">step   90 /  500 | loss 2.2634</span><br><span class="line">step  100 /  500 | loss 3.6054</span><br><span class="line">step  110 /  500 | loss 2.5797</span><br><span class="line">step  120 /  500 | loss 2.6780</span><br><span class="line">step  130 /  500 | loss 3.0241</span><br><span class="line">step  140 /  500 | loss 2.3444</span><br><span class="line">step  150 /  500 | loss 2.7279</span><br><span class="line">step  160 /  500 | loss 2.0370</span><br><span class="line">step  170 /  500 | loss 2.5678</span><br><span class="line">step  180 /  500 | loss 2.3344</span><br><span class="line">step  190 /  500 | loss 2.9308</span><br><span class="line">step  200 /  500 | loss 2.2599</span><br><span class="line">step  210 /  500 | loss 2.4999</span><br><span class="line">step  220 /  500 | loss 2.4038</span><br><span class="line">step  230 /  500 | loss 1.9947</span><br><span class="line">step  240 /  500 | loss 3.8698</span><br><span class="line">step  250 /  500 | loss 2.3555</span><br><span class="line">step  260 /  500 | loss 1.9028</span><br><span class="line">step  270 /  500 | loss 2.6435</span><br><span class="line">step  280 /  500 | loss 2.0100</span><br><span class="line">step  290 /  500 | loss 2.2278</span><br><span class="line">step  300 /  500 | loss 2.3347</span><br><span class="line">step  310 /  500 | loss 2.3856</span><br><span class="line">step  320 /  500 | loss 1.8774</span><br><span class="line">step  330 /  500 | loss 2.5025</span><br><span class="line">step  340 /  500 | loss 2.0494</span><br><span class="line">step  350 /  500 | loss 2.2334</span><br><span class="line">step  360 /  500 | loss 2.4822</span><br><span class="line">step  370 /  500 | loss 2.0925</span><br><span class="line">step  380 /  500 | loss 2.1807</span><br><span class="line">step  390 /  500 | loss 3.5271</span><br><span class="line">step  400 /  500 | loss 2.2285</span><br><span class="line">step  410 /  500 | loss 2.8391</span><br><span class="line">step  420 /  500 | loss 2.6694</span><br><span class="line">step  430 /  500 | loss 2.1652</span><br><span class="line">step  440 /  500 | loss 2.3770</span><br><span class="line">step  450 /  500 | loss 2.8947</span><br><span class="line">step  460 /  500 | loss 2.3532</span><br><span class="line">step  470 /  500 | loss 3.9840</span><br><span class="line">step  480 /  500 | loss 2.0096</span><br><span class="line">step  490 /  500 | loss 2.4628</span><br><span class="line">step  500 /  500 | loss 2.0160</span><br><span class="line"></span><br><span class="line">✅ 模型已保存到: gpt_trained_model.pkl</span><br><span class="line"></span><br><span class="line">--- 开始推理生成文本 ---</span><br><span class="line">样本  1: lellen</span><br><span class="line">样本  2: keles</span><br><span class="line">样本  3: aylera</span><br><span class="line">样本  4: kellone</span><br><span class="line">样本  5: aman</span><br><span class="line">样本  6: lela</span><br><span class="line">样本  7: ameri</span><br><span class="line">样本  8: kan</span><br><span class="line">样本  9: nareena</span><br><span class="line">样本 10: aliela</span><br><span class="line">样本 11: seyn</span><br><span class="line">样本 12: daman</span><br><span class="line">样本 13: caaren</span><br><span class="line">样本 14: ozyren</span><br><span class="line">样本 15: kahiea</span><br><span class="line">样本 16: anytte</span><br><span class="line">样本 17: shilol</span><br><span class="line">样本 18: deler</span><br><span class="line">样本 19: azele</span><br><span class="line">样本 20: maton</span><br></pre></td></tr></table></figure></div>


		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> [Github-Repos] microgpt</li>
        <li><strong>作者:</strong> HackC0der</li>
        <li><strong>创建于
                :</strong> 2026-02-13 09:09:47</li>
        
            <li>
                <strong>更新于
                    :</strong> 2026-02-14 10:14:27
            </li>
        
        <li>
            <strong>链接:</strong> https://hackc0der.github.io/2026/02/13/Github-Repos-microgpt/
        </li>
        <li>
            <strong>
                版权声明:
            </strong>
            

            
                本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a> 进行许可。
            
        </li>
    </ul>
</div>

		</div>
		

		
		<ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
			
			<li class="tag-item mx-0.5">
				<a href="/tags/Interesting-Github-Repos/">#Interesting Github Repos</a>&nbsp;
			</li>
			
		</ul>
		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/2026/01/18/Linux-Kernel-PWN%EF%BC%88%E4%BA%8C%EF%BC%89Kernel-ROP/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">[Linux-Kernel-PWN] Kernel ROP</span>
						<span class="post-nav-item">下一篇</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">目录</div>
		<div class="page-title">[Github-Repos] microgpt</div>
		<ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81README"><span class="nav-text">一、README</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81What-does-this-repo-have-done"><span class="nav-text">二、What does this repo have done?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Load-the-dataset"><span class="nav-text">Load the dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Build-a-tokenizer"><span class="nav-text">Build a tokenizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implement-autograd-from-scratch"><span class="nav-text">Implement autograd from scratch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Initialize-model-parameters"><span class="nav-text">Initialize model parameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Define-the-forward-pass"><span class="nav-text">Define the forward pass</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81How-to-run-microgpt"><span class="nav-text">三、How to run microgpt?</span></a></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2025</span>
              -
            
            2026&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">HackC0der</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        共撰写了 6 篇文章
                    </span>
                    
                        <span>
                            共 49.8k 字
                        </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">访问人数</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">总访问量</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span>
            <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.5</a></span>
        </div>
        
        
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	

</main>



<script src="/js/build/libs/Swup.min.js"></script>

<script src="/js/build/libs/SwupSlideTheme.min.js"></script>

<script src="/js/build/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/build/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/build/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/build/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	
<script src="/js/build/tools/imageViewer.js" type="module"></script>

<script src="/js/build/utils.js" type="module"></script>

<script src="/js/build/main.js" type="module"></script>

<script src="/js/build/layouts/navbarShrink.js" type="module"></script>

<script src="/js/build/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/build/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/build/layouts/categoryList.js" type="module"></script>





    
<script src="/js/build/tools/codeBlock.js" type="module"></script>




    
<script src="/js/build/layouts/lazyload.js" type="module"></script>




    
<script src="/js/build/tools/runtime.js"></script>

    
<script src="/js/build/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/build/libs/Typed.min.js"></script>

  
<script src="/js/build/plugins/typed.js" type="module"></script>











    
<script src="/js/build/tools/tocToggle.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/layouts/toc.js" type="module" data-swup-reload-script=""></script>

<script src="/js/build/plugins/tabs.js" type="module" data-swup-reload-script=""></script>




<script src="/js/build/libs/moment-with-locales.min.js" data-swup-reload-script=""></script>


<script src="/js/build/layouts/essays.js" type="module" data-swup-reload-script=""></script>





	
</body>

</html>